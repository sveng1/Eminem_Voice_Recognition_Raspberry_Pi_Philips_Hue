{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, GRU, Permute, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from google.colab import drive\n",
    "from model import crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_COLAB = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrograms_and_labels(slice_len):\n",
    "\n",
    "    image_filename = 'specs_{}_sec.npy'.format(slice_len)\n",
    "    label_filename = 'labels_{}_sec.npy'.format(slice_len)\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        data_folder = '/content/drive/ML_with_Eminem/data/'\n",
    "        print('Loading data from google drive\\n')\n",
    "        \n",
    "    except:\n",
    "        data_folder = 'data/'\n",
    "        print('Loading data from local directory\\n')\n",
    "        \n",
    "    X = np.load(data_folder+image_filename)\n",
    "    y = np.load(data_folder+label_filename)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=True)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_lens = [3]\n",
    "first_conv_size = [32,64]\n",
    "other_convs_size = [64, 128, 256]\n",
    "gru_size = [32, 64]\n",
    "dense_activation = ['softmax', 'sigmoid']\n",
    "learning_rate = [0.001, 0.0001]\n",
    "batch_size = [8, 16, 32]\n",
    "hyperparams = [first_conv_size, other_convs_size, gru_size, dense_activation, learning_rate, batch_size]\n",
    "hyper_combos = list(itertools.product(*hyperparams))\n",
    "len(hyper_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_folder = 'weights/'\n",
    "\n",
    "runs = pd.DataFrame(columns=['slice_len','first_conv_size', 'other_convs_size', 'gru_size', 'dense_activation',\n",
    "                             'learning_rate','batch_size', 'train_accuracy', 'val_accuracy', 'train_loss',\n",
    "                             'val_loss', 'model path'])\n",
    "\n",
    "for slice_len in slice_lens:\n",
    "    print('Slice length: {} seconds'.format(slice_len))\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_test, y_test, data_folder = load_spectrograms_and_labels(slice_len)\n",
    "    \n",
    "    # Save test data for later testing\n",
    "    np.save(data_folder+'spectrograms_test_{}sec.npy'.format(slice_len),X_test)\n",
    "    np.save(data_folder+'labels_test_{}_sec.npy'.format(slice_len),y_test)\n",
    "    \n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "    print('input shape:', input_shape)\n",
    "    \n",
    "    for i in range(len(hyper_combos)):\n",
    "\n",
    "        # Hyperparameters\n",
    "        p = hyper_combos[i]\n",
    "        weights = weights_folder + 'filter1{}_filter2{}_GRU{}_dense_act{}_lr{}_batch{}'.format(p[0],p[1],p[2],p[3],\n",
    "                                                                                               p[4],p[5])\n",
    "        print(weights)\n",
    "        # Create model\n",
    "        model = crnn(first_conv_size=p[0], other_convs_size=p[1], gru_size=p[2],\n",
    "                     dense_activation=p[3], input_shape=input_shape)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=p[4]), metrics=['accuracy'])\n",
    "        checkpointer = ModelCheckpoint(filepath=weights, verbose=1, save_best_only=True)\n",
    "        earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)  \n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(X_train, y_train, batch_size=p[5], epochs=1, validation_split=0.1,\n",
    "                           callbacks=[checkpointer, earlystopper])\n",
    "\n",
    "        # Save information about hyperparameters and results\n",
    "        runs.loc[len(runs)] = [slice_len, p[0],p[1],p[2], p[3], p[4], p[5], history.history['accuracy'][-1],\n",
    "                               history.history['val_accuracy'][-1], history.history['loss'][-1],\n",
    "                               history.history['val_loss'][-1], weights]\n",
    "    \n",
    "runs.to_csv('models/runs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
